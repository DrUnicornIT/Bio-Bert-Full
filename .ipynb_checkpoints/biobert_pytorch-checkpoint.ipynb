{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9DkuOr0Cp2Rs",
    "outputId": "e0b84603-2401-4c18-b6db-2f5a30bdb379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R2HIrCuLp2Rv",
    "outputId": "a1bf8595-e7f8-4561-d6a6-0374cc9e5b9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib \n",
    "# matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "%pip install tqdm\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9wz79ajdp2Rx",
    "outputId": "4cc285b6-0427-43bc-9b29-caa113f8ddfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(0)\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "d8Owr1SoKiZ0"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"./ChemProt.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zi2nMtRp2Ry",
    "outputId": "0f162e6b-ea95-4f71-9cb2-e4023a5854b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"dmis-lab/biobert-base-cased-v1.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aXQ2Pmz8p2Rz",
    "outputId": "77bcbf61-e015-4e2d-f4f3-deadd6e77bd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='dmis-lab/biobert-base-cased-v1.2', vocab_size=28996, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ulaYRKJXp2R0"
   },
   "outputs": [],
   "source": [
    "task_name = \"chemprot\"\n",
    "data_dir = \"./Data/ChemProt/Contrastive/\"\n",
    "model_type = \"cl_pretraining\"\n",
    "max_seq_le = 128\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "gILA4tPpp2R0"
   },
   "outputs": [],
   "source": [
    "class DataContrastive(object):\n",
    "    def _read_tsv(clm, input_file):\n",
    "        data = []\n",
    "        with open(input_file, 'r') as f:\n",
    "            for line in f:\n",
    "                fields = line.strip().split('\\t')\n",
    "                data.append([fields[0], fields[1]])\n",
    "                data.append([fields[0], fields[2]])\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zzW4muF1p2R1"
   },
   "outputs": [],
   "source": [
    "class BioBERTChemprotProcessor(DataContrastive):\n",
    "    def get_train_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")))\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "    def get_test_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "    def get_labels(self):\n",
    "        return [\"CPR:3\", \"CPR:4\", \"CPR:5\", \"CPR:6\", \"CPR:9\", \"false\"]\n",
    "    def _create_examples(self, lines):\n",
    "        examples = []\n",
    "        for (i, line) in tqdm(enumerate(lines)):\n",
    "            inputs = tokenizer(line[1], padding = 'max_length', truncation=True, max_length = max_seq_le, return_tensors = 'pt')\n",
    "            label = torch.tensor(0)\n",
    "            examples.append([inputs['input_ids'], inputs['attention_mask'], label])\n",
    "        return examples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "V0mKuvu_p2R2"
   },
   "outputs": [],
   "source": [
    "class ChemProtContrastivePreTraining(nn.Module):\n",
    "    def __init__(self, model, tokenizer):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(model.config.hidden_size, model.config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(model.config.hidden_size, model.config.hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        projections = self.projection_head(embeddings)\n",
    "        return projections\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-wtDRIxp2R3",
    "outputId": "29eb8c98-2125-4960-9783-5f1a506e097e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71000it [00:41, 1724.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: chemprot\n",
      "List label: \n",
      "['CPR:3', 'CPR:4', 'CPR:5', 'CPR:6', 'CPR:9', 'false']\n",
      "[tensor([[  101,  1106,  1233,  2497, 21919,  1179,  1110,   170, 14930,   137,\n",
      "          5297,   109,   170,  1964,  1643,   114,   191,   113,   123,   114,\n",
      "         10814,  3510,  1200,  1215,  1106, 21497,  1714,  1447,  4267, 10374,\n",
      "          1548,  1107,  1103,  3252,  1104,   174,  1358,  6005, 14183,  1596,\n",
      "          1137,   177, 24312,  6005, 14183,  1596,   177,  1183,  5674, 24226,\n",
      "         16996,  1465,   119,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), tensor(0)]\n"
     ]
    }
   ],
   "source": [
    "processors = {\n",
    "      \"chemprot\": BioBERTChemprotProcessor,\n",
    "}\n",
    "models = {\n",
    "    \"cl_pretraining\": ChemProtContrastivePreTraining\n",
    "}\n",
    "processor = processors[task_name]()\n",
    "label_list = processor.get_labels()\n",
    "train_contrastive = processor.get_train_examples(data_dir)\n",
    "model_fn = models[model_type]\n",
    "\n",
    "print(\"Task: \" + str(task_name))\n",
    "print(\"List label: \")\n",
    "print(label_list)\n",
    "print(train_contrastive[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIm5AZ-Dp2R4"
   },
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "A9lrqfEbp2R5"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "max_seq_len_pp = 128\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \n",
    "\n",
    "    inputs = [item[0] for item in batch]\n",
    "    attn_masks = [item[1] for item in batch]\n",
    "    labels = [item[2] for item in batch]\n",
    "    \n",
    "    max_seq_len = max(max_seq_len_pp, max([int(seq.size(1)) for seq in inputs]))\n",
    "    inputs = [torch.nn.functional.pad(seq, (0, max_seq_len - seq.size(1)), value=0) for seq in inputs]\n",
    "    attn_masks = [torch.nn.functional.pad(seq, (0, max_seq_len - seq.size(1)), value=0) for seq in attn_masks]\n",
    "    \n",
    "    inputs = pad_sequence(inputs, batch_first=True)\n",
    "    attn_masks = pad_sequence(attn_masks, batch_first=True)\n",
    "    labels = torch.stack(labels)\n",
    "    \n",
    "    return inputs, attn_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "xafhAHq7p2R5"
   },
   "outputs": [],
   "source": [
    "train_contrastive_dataloader = DataLoader(train_contrastive, batch_size=batch_size, shuffle=True, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JJpGEczYp2R6",
    "outputId": "2c7dc6ba-58cf-4155-bc0a-64ecd005812a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  101,  1113,  1103,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  3336, 17030,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  2456,   117,  ...,     0,     0,     0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  101,  1195,  1276,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  1103,  2281,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  1165,  1821,  ...,   117,   174,   102]]])\n",
      "tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 1, 1, 1]]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for inputs, atention_mask, labels in train_contrastive_dataloader: \n",
    "    print(inputs)\n",
    "    print(atention_mask)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Y8vKQ74p2R6",
    "outputId": "31d27d97-08b2-4d78-fdd7-a346b63ee0a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = model_fn(model, tokenizer)\n",
    "model_run = model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "DONJaxldp2R6"
   },
   "outputs": [],
   "source": [
    "def cal_loss(output_layer): \n",
    "\n",
    "        cosine_sim_1d = tf.keras.losses.CosineSimilarity(axis=1,)\n",
    "        cosine_sim_2d = tf.keras.losses.CosineSimilarity(axis=2,)\n",
    "\n",
    "\n",
    "        def _cosine_simililarity_dim1(x, y):\n",
    "            v = cosine_sim_1d(x, y)\n",
    "            return v\n",
    "\n",
    "\n",
    "        def _cosine_simililarity_dim2(x, y):\n",
    "            v = cosine_sim_2d(tf.expand_dims(x, 1), tf.expand_dims(y, 0))\n",
    "            return v\n",
    "\n",
    "\n",
    "        def _dot_simililarity_dim1(x, y):\n",
    "            v = tf.matmul(tf.expand_dims(x, 1), tf.expand_dims(y, 2))\n",
    "            return v\n",
    "\n",
    "\n",
    "        def _dot_simililarity_dim2(x, y):\n",
    "            v = tf.tensordot(tf.expand_dims(x, 1), tf.expand_dims(tf.transpose(y), 0), axes=2)\n",
    "            return v\n",
    "\n",
    "        def get_negative_mask(batch_size):\n",
    "            negative_mask = np.ones((batch_size, 2 * batch_size), dtype=bool)\n",
    "            for i in range(batch_size):\n",
    "                negative_mask[i, i] = 0\n",
    "                negative_mask[i, i + batch_size] = 0\n",
    "            return tf.constant(negative_mask)\n",
    "        \n",
    "        criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,)\n",
    "        negative_mask = get_negative_mask(int(batch_size/2))\n",
    "        output_layer = output_layer.cpu()\n",
    "        output_layer = output_layer.detach().numpy()\n",
    "        zis = output_layer[::2] #z0 z2 z4\n",
    "        zjs = output_layer[1::2] # z1 z3 z5\n",
    "\n",
    "        zis = tf.math.l2_normalize(zis, axis=1)\n",
    "        zjs = tf.math.l2_normalize(zjs, axis=1)\n",
    "        l_pos = _dot_simililarity_dim1(zis, zjs)\n",
    "        l_pos = tf.reshape(l_pos, (int(batch_size/2), 1))\n",
    "        l_pos /= 0.1\n",
    "\n",
    "        negatives = tf.concat([zjs, zis], axis=0)\n",
    "        loss = 0\n",
    "\n",
    "        for positives in [zis, zjs]:\n",
    "          l_neg = _dot_simililarity_dim2(positives, negatives)\n",
    "\n",
    "          labels = tf.zeros(int(batch_size/2), dtype=tf.int32)\n",
    "\n",
    "          l_neg = tf.boolean_mask(l_neg, negative_mask)\n",
    "          l_neg = tf.reshape(l_neg, (int(batch_size/2), -1))\n",
    "          l_neg /= 0.1\n",
    "          # assert l_neg.shape == (\n",
    "          #     config['batch_size'], 2 * (config['batch_size'] - 1)), \"Shape of negatives not expected.\" + str(\n",
    "          #     l_neg.shape)\n",
    "          logits = tf.concat([l_pos, l_neg], axis=1)  # [N,K+1]\n",
    "          loss += criterion(y_pred=logits, y_true=labels)\n",
    "          #print(loss)\n",
    "        \n",
    "        loss_tf = loss / (batch_size)\n",
    "        #print(\"Done\")\n",
    "        #print(loss_tf)\n",
    "        loss_np = loss_tf.numpy()\n",
    "        loss_torch = torch.tensor(loss_np, dtype = torch.float32,requires_grad=True)\n",
    "\n",
    "        return loss_torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "O0DbQG7Sp2R6"
   },
   "outputs": [],
   "source": [
    "def ContrastiveTrain(model, optimizer, train_loader, num_epochs):\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        print('Training Contrastive ...')\n",
    "        for inputs, attention_mask, labels in tqdm(train_loader):\n",
    "            if len(inputs) != batch_size:\n",
    "              break\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            inputs = inputs.squeeze()\n",
    "            attention_mask = attention_mask.squeeze()\n",
    "            labels = labels.squeeze()\n",
    "            \n",
    "            outputs = model(input_ids=inputs, attention_mask=attention_mask)\n",
    "            loss = cal_loss(outputs)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u2dmmpEVp2R7",
    "outputId": "2cd8ad6b-8ff3-4d1a-ee5d-ad2411e20880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Training Contrastive ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1109/1110 [08:48<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "Training Contrastive ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1109/1110 [08:49<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "Training Contrastive ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1109/1110 [08:48<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "Training Contrastive ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1109/1110 [08:48<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "Training Contrastive ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1109/1110 [08:48<00:00,  2.10it/s]\n"
     ]
    }
   ],
   "source": [
    "model_extra_pretrain = ContrastiveTrain(model_run, optimizer, train_contrastive_dataloader, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIKSyo06Azi4"
   },
   "source": [
    "# Get datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "-LlX9GykfRGj"
   },
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': 10,\n",
    "            'model_state_dict': model_extra_pretrain.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "            }, 'Model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ME4nkhpuAkr5"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./Data/ChemProt/Main/train.tsv', sep=\"\\t\")\n",
    "\n",
    "dev_data = pd.read_csv('./Data/ChemProt/Main/dev.tsv', sep=\"\\t\")\n",
    "\n",
    "test_data = pd.read_csv('./Data/ChemProt/Main/test.tsv', sep=\"\\t\")\n",
    "\n",
    "\n",
    "train_data_aug = pd.read_csv('./Data/ChemProt/Aug/train.tsv', sep=\"\\t\")\n",
    "\n",
    "dev_data_aug = pd.read_csv('./Data/ChemProt/Aug/dev.tsv', sep=\"\\t\")\n",
    "\n",
    "test_data_aug = pd.read_csv('./Data/ChemProt/Aug/test.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "N4kCNULoA-hu"
   },
   "outputs": [],
   "source": [
    "train_data = train_data.dropna(subset=[\"index\", \"sentence\", \"label\"])\n",
    "dev_data = dev_data.dropna(subset=[\"index\", \"sentence\", \"label\"])\n",
    "test_data = test_data.dropna(subset=[\"index\", \"sentence\", \"label\"])\n",
    "\n",
    "train_data_aug = train_data_aug.dropna(subset=[\"index\", \"sentence\", \"label\"])\n",
    "dev_data_aug = dev_data_aug.dropna(subset=[\"index\", \"sentence\", \"label\"])\n",
    "test_data_aug = test_data_aug.dropna(subset=[\"index\", \"sentence\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "V8BB5yKoCAAh"
   },
   "outputs": [],
   "source": [
    "labels = ['false', 'CPR:3', 'CPR:4', 'CPR:5', 'CPR:6', 'CPR:9']\n",
    "label_map = {label: i for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "2oVulcq7ChfI"
   },
   "outputs": [],
   "source": [
    "class ChemProtDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, label_map):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_map = label_map\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.data.iloc[index]['sentence']\n",
    "        label = self.data.iloc[index]['label']\n",
    "        label_id = self.label_map[label]\n",
    "        inputs = self.tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "        return inputs['input_ids'], inputs['attention_mask'], torch.tensor(label_id, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lCXKSsW4CSsU",
    "outputId": "ef5d2c61-226b-4013-b1e4-897ac05b978b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin --\n",
      "Length of Train Dataset: 18035\n",
      "Length of Dev-set: 11268\n",
      "Length of Tests: 15745\n",
      "\n",
      "\n",
      "Aug --\n",
      "Length of Train Dataset: 18035\n",
      "Length of Dev-set: 11268\n",
      "Length of Tests: 31490\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ChemProtDataset(train_data, tokenizer, label_map)\n",
    "dev_dataset = ChemProtDataset(dev_data, tokenizer, label_map)\n",
    "test_dataset = ChemProtDataset(test_data, tokenizer, label_map)\n",
    "\n",
    "train_dataset_aug = ChemProtDataset(train_data_aug, tokenizer, label_map)\n",
    "dev_dataset_aug = ChemProtDataset(dev_data_aug, tokenizer, label_map)\n",
    "test_dataset_aug = ChemProtDataset(test_data_aug, tokenizer, label_map)\n",
    "\n",
    "print(\"Origin --\")\n",
    "print(\"Length of Train Dataset: \" + str(len(train_dataset)))\n",
    "print(\"Length of Dev-set: \"+ str(len(dev_dataset)))\n",
    "print(\"Length of Tests: \" + str(len(test_dataset)))\n",
    "print('\\n')\n",
    "print(\"Aug --\")\n",
    "print(\"Length of Train Dataset: \" + str(len(train_dataset_aug)))\n",
    "print(\"Length of Dev-set: \"+ str(len(dev_dataset_aug)))\n",
    "print(\"Length of Tests: \" + str(len(test_dataset_aug)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "rWMYrxExCmco"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "max_seq_len_pp = 128\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs = [item[0] for item in batch]\n",
    "    attn_masks = [item[1] for item in batch]\n",
    "    labels = [item[2] for item in batch]\n",
    "    \n",
    "    # Find maximum length of sequences along the second dimension\n",
    "    max_seq_len = max(max_seq_len_pp, max([seq.size(1) for seq in inputs]))\n",
    "    \n",
    "    # Pad sequences with zeros to maximum length\n",
    "    inputs = [torch.nn.functional.pad(seq, (0, max_seq_len - seq.size(1)), value=0) for seq in inputs]\n",
    "    attn_masks = [torch.nn.functional.pad(seq, (0, max_seq_len - seq.size(1)), value=0) for seq in attn_masks]\n",
    "    \n",
    "    # Stack padded sequences\n",
    "    inputs = pad_sequence(inputs, batch_first=True)\n",
    "    attn_masks = pad_sequence(attn_masks, batch_first=True)\n",
    "    labels = torch.stack(labels)\n",
    "    \n",
    "    return inputs, attn_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "P1hB7NDBCobB"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "train_dataloader_aug = DataLoader(train_dataset_aug, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "dev_dataloader_aug = DataLoader(dev_dataset_aug, batch_size=32, collate_fn=collate_fn)\n",
    "test_dataloader_aug = DataLoader(test_dataset_aug, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2QxegXaAvN0"
   },
   "source": [
    "# Augmented evaluation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "xffew0FgEf2u"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
    "def compute_metrics(labels, preds):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"micro\", labels=[0, 1, 2, 3, 4]\n",
    "    )\n",
    "    return {\"micro_precision\": precision, \"micro_recall\": recall, \"micro_f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5oFQEY7z2-EQ",
    "outputId": "8a65881c-4559-475a-dffa-d5e6c5b214c0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model_run.parameters(), lr=2e-5, eps=1e-8)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train(model, optimizer, criterion, train_loader, val_loader, test_loader, num_epochs):\n",
    "    print(\"HMM\")\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        print('Traininggg ...')\n",
    "        for inputs, attention_mask, labels in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            inputs = inputs.squeeze()\n",
    "            attention_mask = attention_mask.squeeze()\n",
    "            labels = labels.squeeze()\n",
    "            \n",
    "            outputs = model(input_ids=inputs, attention_mask=attention_mask)\n",
    "\n",
    "            loss = criterion(outputs,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = train_loss / len(train_loader.dataset)\n",
    "        print(\"Train loss: \" + str(epoch_loss))\n",
    "            \n",
    "            \n",
    "    \n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    val_true = []\n",
    "    val_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, attention_mask, labels in tqdm(val_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            inputs = inputs.squeeze()\n",
    "            attention_mask = attention_mask.squeeze()\n",
    "            labels = labels.squeeze()\n",
    "\n",
    "            outputs = model(input_ids=inputs, attention_mask=attention_mask)\n",
    "\n",
    "            loss = criterion(outputs,labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            pred = torch.argmax(outputs, dim=1)\n",
    "            val_true.extend(labels.cpu().numpy().tolist())\n",
    "            val_pred.extend(pred.cpu().numpy().tolist())\n",
    "    eval_loss = running_loss / len(val_loader.dataset)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(val_true, val_pred, average='micro', labels=[0, 1, 2, 3, 4])\n",
    "    print(\"Eval loss: \", eval_loss) \n",
    "    print(\"Micro Precision:\", precision)\n",
    "    print(\"Micro Recall:\", recall)\n",
    "    print(\"Micro F1 Score:\", f1)\n",
    "    print('\\nTest ...')\n",
    "\n",
    "    test_loss = 0.0\n",
    "    test_true = []\n",
    "    test_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, attention_mask, labels in tqdm(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            inputs = inputs.squeeze(1)\n",
    "            attention_mask = attention_mask.squeeze(1)\n",
    "\n",
    "            outputs = model(input_ids=inputs, attention_mask=attention_mask)\n",
    "\n",
    "            loss = criterion(outputs,labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            pred = torch.argmax(outputs, dim=1)\n",
    "            test_true.extend(labels.cpu().numpy().tolist())\n",
    "            test_pred.extend(pred.cpu().numpy().tolist())\n",
    "    test_loss = running_loss / len(val_loader.dataset)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(test_true, test_pred, average='micro', labels=[0, 1, 2, 3, 4])\n",
    "    print(\"Eval loss: \", test_loss) \n",
    "    print(\"Micro Precision:\", precision)\n",
    "    print(\"Micro Recall:\", recall)\n",
    "    print(\"Micro F1 Score:\", f1)\n",
    "    print('\\nTest ...')\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sni5L5st3D7l",
    "outputId": "e3d8d509-9649-4eab-c20c-94c32d78b909"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM\n",
      "Epoch 1/8\n",
      "Traininggg ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 564/564 [08:42<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.9115354691412365\n",
      "Epoch 2/8\n",
      "Traininggg ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 564/564 [08:42<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2248274740962596\n",
      "Epoch 3/8\n",
      "Traininggg ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 564/564 [08:44<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1393579354453424\n",
      "Epoch 4/8\n",
      "Traininggg ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 564/564 [08:44<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0959032511856538\n",
      "Epoch 5/8\n",
      "Traininggg ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 564/564 [08:39<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07115332623755201\n",
      "Epoch 6/8\n",
      "Traininggg ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 564/564 [08:41<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.05295751909424567\n",
      "Epoch 7/8\n",
      "Traininggg ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 564/564 [08:40<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.04708058533333872\n",
      "Epoch 8/8\n",
      "Traininggg ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 564/564 [08:42<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.040905041344688975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 353/353 [01:25<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss:  0.4479942017119645\n",
      "Micro Precision: 0.9073366272023865\n",
      "Micro Recall: 0.9002867449819628\n",
      "Micro F1 Score: 0.9037979385272541\n",
      "\n",
      "Test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 985/985 [04:09<00:00,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loss:  1.9122408981748396\n",
      "Micro Precision: 0.8985901476198375\n",
      "Micro Recall: 0.8967754750711779\n",
      "Micro F1 Score: 0.8976818942519593\n",
      "\n",
      "Test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = train(model_run, optimizer, criterion, train_dataloader_aug, dev_dataloader_aug, test_dataloader_aug, num_epochs=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 132
    },
    "id": "zqeTgVZuC40_",
    "outputId": "647e36f0-024b-415d-a157-9084ae301338"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-35-268482e0db84>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    val_loss = 0.0\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
